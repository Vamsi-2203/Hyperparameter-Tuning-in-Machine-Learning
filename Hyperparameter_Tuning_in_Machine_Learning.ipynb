{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i41A7cvp1ss0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter Tuning in Machine Learning\n",
        "\n",
        "\n",
        "*   Hyperparameter tuning is the process of selecting the best set of hyperparameters to improve model performance.\n",
        "*   Hyperparameter tuning is a crucial step in improving the performance of machine learning models.\n",
        "\n",
        "*   Various algorithms benefit from optimized hyperparameters to achieve better accuracy, generalization, and efficiency.\n",
        "\n",
        "\n",
        "*   Below is a list of popular algorithms that utilize hyperparameter tuning:\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZKrN1mQW2Cdm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Manual Search\n",
        "\n",
        "### How It Works:\n",
        "\n",
        "Manually choose combinations of hyperparameters based on intuition or experience.\n",
        "Train and evaluate the model for each combination.\n",
        "\n",
        "### Advantages:\n",
        "\n",
        "Simple and quick for small parameter spaces.\n",
        "\n",
        "### Disadvantages:\n",
        "\n",
        "Time-consuming and inefficient for large parameter spaces.\n",
        "Might miss the optimal combination."
      ],
      "metadata": {
        "id": "p5FDIb8V3Emg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Grid Search\n",
        "### How It Works:\n",
        "\n",
        "Exhaustively searches through a specified set of hyperparameter combinations.\n",
        "For each combination, the model is trained and evaluated.\n",
        "\n",
        "### Advantages:\n",
        "\n",
        "Ensures all combinations are tested.\n",
        "\n",
        "### Disadvantages:\n",
        "\n",
        "Computationally expensive for large parameter spaces."
      ],
      "metadata": {
        "id": "lyXD7Z1g3TuX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Define the model\n",
        "model = RandomForestClassifier()\n",
        "\n",
        "# Define hyperparameters to search\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [10, 20, None],\n",
        "    'max_features': ['sqrt', 'log2', None]\n",
        "}\n",
        "\n",
        "# Perform Grid Search\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n"
      ],
      "metadata": {
        "id": "r1hTprlK2Efw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Random Search\n",
        "### How It Works:\n",
        "\n",
        "Randomly samples combinations of hyperparameters from the specified parameter distributions.\n",
        "A fixed number of combinations is evaluated.\n",
        "\n",
        "### Advantages:\n",
        "\n",
        "Faster than grid search for large spaces.\n",
        "Can find good hyperparameters without testing all combinations.\n",
        "\n",
        "### Disadvantages:\n",
        "\n",
        "Results can vary due to randomness.\n"
      ],
      "metadata": {
        "id": "1-cDtvua3khG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Define the model\n",
        "model = RandomForestClassifier()\n",
        "\n",
        "# Define hyperparameters to search\n",
        "param_dist = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [10, 20, None],\n",
        "    'max_features': ['sqrt', 'log2', None],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "# Perform Random Search\n",
        "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=20, cv=5, random_state=42)\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Hyperparameters:\", random_search.best_params_)\n"
      ],
      "metadata": {
        "id": "adbEpCgA3j_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Bayesian Optimization\n",
        "### How It Works:\n",
        "\n",
        "Uses probabilistic models to predict the performance of hyperparameter combinations.\n",
        "Focuses on the most promising areas of the parameter space.\n",
        "\n",
        "### Advantages:\n",
        "\n",
        "Efficient exploration of the parameter space.\n",
        "Often finds optimal or near-optimal parameters faster than grid or random search.\n",
        "\n",
        "### Disadvantages:\n",
        "\n",
        "More complex to implement.\n"
      ],
      "metadata": {
        "id": "Ch2YL4TM3uDY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from skopt import BayesSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Define the model\n",
        "model = RandomForestClassifier()\n",
        "\n",
        "# Define hyperparameters to search\n",
        "param_dist = {\n",
        "    'n_estimators': (100, 500),\n",
        "    'max_depth': (10, 50),\n",
        "    'max_features': ['sqrt', 'log2', None],\n",
        "}\n",
        "\n",
        "# Perform Bayesian Search\n",
        "bayes_search = BayesSearchCV(estimator=model, search_spaces=param_dist, n_iter=20, cv=5, random_state=42)\n",
        "bayes_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Hyperparameters:\", bayes_search.best_params_)\n"
      ],
      "metadata": {
        "id": "Ir4BlxJG39vk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Best Practices for Hyperparameter Tuning\n",
        "\n",
        "\n",
        "1.   Start with random search or grid search for simple problems.\n",
        "2.   Use Bayesian optimization for complex problems and large datasets.\n",
        "3.   Evaluate using cross-validation to ensure the model generalizes well.\n",
        "4.   Monitor resource usage to prevent excessive computation time.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-kJ_7dgf4G64"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IBkiW6Dc4Baf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}